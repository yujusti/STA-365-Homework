{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eba01a9-d0ef-4e7a-a176-cd5b5a178bd1",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a59bf-0aaa-45a0-b68c-caf53b8fc2fe",
   "metadata": {},
   "source": [
    "Student: Justin Yu\n",
    "\n",
    "Student Number: 1006747111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b11fba-3666-4211-b2e0-d5ce89ebc98a",
   "metadata": {},
   "source": [
    "## 1. Describe how the posterior predictive distribution is created for mixture models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e796674-6074-41aa-92f9-24c29434adbe",
   "metadata": {},
   "source": [
    "After fitting a Bayesian Mixture model, the posterior predictive distribution is created by randomly sampling from the drawn samples and chains for j repetitions (the more samples, the better). For each random chain and drawn sample selected from the model, the estimated parameters are given for each component, and using these estimated parameters, we can create the probability distribution functions for each component in the mixture model. From this, we obtain 'h' probability distribution functions for each component, which we then multiply by the weights of each component and sum all the component's pdfs to give a combined pdf for one randomly selected draw and chain. After j repetitions, the average combined probability distribution across all the j-selected samples is then calculated, giving us the posterior predictive distribution for the mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b027ad4-d212-452b-973f-bcf0da938fe9",
   "metadata": {},
   "source": [
    "## 2. Describe how the posterior predictive distribution is created in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aceee2-d71d-4dd7-8396-81dfbcc8fc84",
   "metadata": {},
   "source": [
    "In general, the posterior predictive distribution of $\\tilde{x}$ given $X$ is calculated by marginalizing the distribution of $\\tilde{x}$ given $\\theta$ over the posterior distribution of $\\theta$ given $X$. This can be written in the form shown below:\n",
    "\n",
    "\n",
    "$$p(\\tilde{x}|X)=\\int p(\\tilde{x}|\\theta)p(\\theta|X)d\\theta$$\n",
    "\n",
    "where $X$ is data collected, we can then find the posterior distribution of $p(\\theta|X)$ using the priors (specified) and likelihood distributions. Recall that the posterior distribution of $p(\\theta|X)$ can be sampled using MCMC and is given by Bayes theorem as:\n",
    "\n",
    "$$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{p(X)}$$\n",
    "\n",
    "We then use those $\\theta$ parameters to generate new sets of data or $p(\\tilde{x}|\\theta)$. This was discussed in Q1, where the estimated parameters $p(\\theta|X)$ are used to create probability distribution functions of $\\tilde{x}$. The posterior predictive distribution $p(\\tilde{x}|X)$ is thus the integral over all possible values of $\\theta$ parameters (similar to that of the weighted average combined probability distribution across all the j-selected samples in Q1). A good point to note is that we can use MCMC or other sampling methods if the integral is not easily solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ab6a8-24e3-4286-9718-98ebafe54a3b",
   "metadata": {},
   "source": [
    "## 3. How to perform a Bayesian analysis without throwing away the rows with missing values in $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda10ff-22cd-4c67-86c3-9574672e971b",
   "metadata": {},
   "source": [
    "There are three fundamental ideas of categorizing missing data:\n",
    "\n",
    "- Missing Completely at Random (MCAR). The probability of a data point being missing is unrelated to both observed and unobserved data.\n",
    "- Missing at Random (MAR). The data's missingness can be a function of the observed data and circumstances of the world. This can be ignored and the missing data can be removed since it does not affect the inference made.\n",
    "- Missing Not at Random (MNAR). The probability of a data point being missing depends on the unobserved (missing) values themselves, even after accounting for the observed data, and is a case of non-ignorable missingness. To account for the uncertainty related to this type of missing data we can employ the following:\n",
    "\n",
    "We can extend the Bayesian model to include latent variables $v$ that indicate whether each observation is missing or observed. These latent variables (with specified prior distributions) are binary indicators where $v_i$ = 1 if the i-th observation is observed and $v_i$ = 0 if the observation is missing. We can then use Markov Chain Monte Carlo (MCMC) sampling, to obtain samples from the joint distribution of all model parameters, including the latent variables. Using the procedures discussed in Q2 and Q1, we can then create the posterior predictive distribution (or sample from it using MCMC) and use it to estimate the missing data using the available observed data (this leads to more accurate inferencing than simply discarding incomplete observations). It is important to assess that the Missing Completely at Random (MCAR) assumption holds during this process as it influences the validity of the inferences and conclusions drawn from the analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
